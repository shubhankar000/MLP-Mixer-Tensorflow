{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Example illustrating how to use the MLP-Mixer API\n",
    "\n",
    "This example also shows the performance of the MLP-Mixer architecture on the CIFAR-10 dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.layers.experimental.preprocessing import (\n",
    "    RandomFlip,\n",
    "    RandomRotation,\n",
    "    RandomZoom,\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from mlp_utils import *\n",
    "\n",
    "# Uncomment line below to run on CPU\n",
    "# tf.config.set_visible_devices([], \"GPU\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-07-21 20:49:33.260588: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Load the dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# normalize pixel values\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n",
    "\n",
    "# create validation data from test data\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "\n",
    "# Define the classes\n",
    "classes = [\n",
    "    \"airplane\",\n",
    "    \"automobile\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    "]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def create_model():\n",
    "    dims = 512\n",
    "    k, s = (4, 4)  # (kernel, strides)\n",
    "    depth = 4\n",
    "\n",
    "    inputs = Input((32, 32, 3))\n",
    "\n",
    "    x = tf.keras.Sequential(\n",
    "        [\n",
    "            RandomFlip(),\n",
    "            RandomRotation(factor=0.03),\n",
    "            RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "        ],\n",
    "        name=\"data_augmentation\",\n",
    "    )(inputs)\n",
    "\n",
    "    x = CreatePatches(k, s)(x)\n",
    "    x = PerPatchFullyConnected(dims)(x)\n",
    "\n",
    "    for _ in range(depth):\n",
    "        x = MLPBlock()(x)\n",
    "\n",
    "    # x = Projection()(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # classification layer\n",
    "    x = GaussianDropout(0.9)(x)\n",
    "    output = Dense(len(classes), activation=\"softmax\", kernel_regularizer=\"l2\")(x)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=output)\n",
    "\n",
    "\n",
    "model = create_model()\n",
    "print(model.summary())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "data_augmentation (Sequentia (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "create_patches (CreatePatche (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "per_patch_fully_connected (P (None, 64, 512)           25088     \n",
      "_________________________________________________________________\n",
      "mlp_block (MLPBlock)         (None, 64, 512)           535680    \n",
      "_________________________________________________________________\n",
      "mlp_block_1 (MLPBlock)       (None, 64, 512)           535680    \n",
      "_________________________________________________________________\n",
      "mlp_block_2 (MLPBlock)       (None, 64, 512)           535680    \n",
      "_________________________________________________________________\n",
      "mlp_block_3 (MLPBlock)       (None, 64, 512)           535680    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_dropout (GaussianDr (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 2,172,938\n",
      "Trainable params: 2,172,938\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Explanation of the layers\n",
    "- The `CreatePatches` layer creates non-overlapping patches of an image using the `tf.image.extract_patches` method. The kernel and stride are provided to ensure the patches are sufficiently small and non-overlapping\n",
    "- The `PerPatchFullyConnected` layer applies a `Dense` layer to all the individual patches of the image, projecting them into `dims` sized vector. This creates our \"Patches x Channels\" table as described in the MLP-Mixer paper.\n",
    "- The MLPBlock takes no arguements because it maintains the input size. It does the channel mixing.\n",
    "\n",
    "Here we are using some specific hyperparameters:\n",
    "- the internal projection dimension `dims` is set to 512\n",
    "- we are using a depth of 4 MLP-blocks\n",
    "- We are using global average pooling as used in the paper\n",
    "- using guassian dropout with a dropout probablity of 0.9 (due to tendency of model to overfit on dataset of this small size, and even the images are small in size)\n",
    "- Adadelta optimizer with a learning rate of 1.0 (as the optimizer manages lr_decay and gives better results the Adam)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adadelta(1),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "b_s = 128 # batch_size\n",
    "\n",
    "val_dataset = val_dataset.batch(b_s)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=val_dataset,\n",
    "    batch_size=b_s,\n",
    "    epochs=100,\n",
    "    verbose=\"auto\",\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-07-21 18:08:37.069792: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-07-21 18:08:37.093193: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 3800060000 Hz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-07-21 18:08:38.790276: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-07-21 18:08:39.132242: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-07-21 18:08:39.132866: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  1/391 [..............................] - ETA: 16:18 - loss: 5.7593 - accuracy: 0.0938"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-07-21 18:08:39.465858: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8202\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "391/391 [==============================] - 33s 78ms/step - loss: 2.3298 - accuracy: 0.2242 - val_loss: 2.0503 - val_accuracy: 0.3082\n",
      "Epoch 2/100\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 1.8273 - accuracy: 0.3505 - val_loss: 1.5902 - val_accuracy: 0.4385\n",
      "Epoch 3/100\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 1.6690 - accuracy: 0.4076 - val_loss: 1.4984 - val_accuracy: 0.4525\n",
      "Epoch 4/100\n",
      "391/391 [==============================] - 29s 75ms/step - loss: 1.5958 - accuracy: 0.4343 - val_loss: 1.5016 - val_accuracy: 0.4619\n",
      "Epoch 5/100\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 1.5404 - accuracy: 0.4581 - val_loss: 1.4481 - val_accuracy: 0.4950\n",
      "Epoch 6/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 1.4845 - accuracy: 0.4771 - val_loss: 1.4189 - val_accuracy: 0.5026\n",
      "Epoch 7/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 1.4463 - accuracy: 0.4950 - val_loss: 1.3596 - val_accuracy: 0.5144\n",
      "Epoch 8/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 1.4116 - accuracy: 0.5113 - val_loss: 1.4664 - val_accuracy: 0.5022\n",
      "Epoch 9/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 1.3786 - accuracy: 0.5232 - val_loss: 1.3399 - val_accuracy: 0.5312\n",
      "Epoch 10/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 1.3505 - accuracy: 0.5346 - val_loss: 1.3028 - val_accuracy: 0.5490\n",
      "Epoch 11/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 1.3254 - accuracy: 0.5467 - val_loss: 1.2613 - val_accuracy: 0.5580\n",
      "Epoch 12/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 1.3038 - accuracy: 0.5529 - val_loss: 1.3046 - val_accuracy: 0.5471\n",
      "Epoch 13/100\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 1.2787 - accuracy: 0.5636 - val_loss: 1.3881 - val_accuracy: 0.5350\n",
      "Epoch 14/100\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 1.2592 - accuracy: 0.5705 - val_loss: 1.4038 - val_accuracy: 0.5367\n",
      "Epoch 15/100\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 1.2441 - accuracy: 0.5754 - val_loss: 1.2896 - val_accuracy: 0.5620\n",
      "Epoch 16/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 1.2205 - accuracy: 0.5866 - val_loss: 1.2481 - val_accuracy: 0.5716\n",
      "Epoch 17/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 1.2019 - accuracy: 0.5925 - val_loss: 1.2020 - val_accuracy: 0.5845\n",
      "Epoch 18/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 1.1776 - accuracy: 0.6020 - val_loss: 1.1527 - val_accuracy: 0.5977\n",
      "Epoch 19/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 1.1635 - accuracy: 0.6064 - val_loss: 1.1759 - val_accuracy: 0.5932\n",
      "Epoch 20/100\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 1.1398 - accuracy: 0.6161 - val_loss: 1.2088 - val_accuracy: 0.5842\n",
      "Epoch 21/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 1.1217 - accuracy: 0.6242 - val_loss: 1.2086 - val_accuracy: 0.5846\n",
      "Epoch 22/100\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 1.1064 - accuracy: 0.6284 - val_loss: 1.1912 - val_accuracy: 0.5911\n",
      "Epoch 23/100\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 1.0949 - accuracy: 0.6327 - val_loss: 1.2014 - val_accuracy: 0.5938\n",
      "Epoch 24/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 1.0656 - accuracy: 0.6410 - val_loss: 1.2263 - val_accuracy: 0.5823\n",
      "Epoch 25/100\n",
      "391/391 [==============================] - 28s 73ms/step - loss: 1.0564 - accuracy: 0.6476 - val_loss: 1.2096 - val_accuracy: 0.5948\n",
      "Epoch 26/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 1.0298 - accuracy: 0.6574 - val_loss: 1.1824 - val_accuracy: 0.6072\n",
      "Epoch 27/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 1.0185 - accuracy: 0.6597 - val_loss: 1.2592 - val_accuracy: 0.5912\n",
      "Epoch 28/100\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.9987 - accuracy: 0.6666 - val_loss: 1.1653 - val_accuracy: 0.6135\n",
      "Epoch 29/100\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.9783 - accuracy: 0.6736 - val_loss: 1.1597 - val_accuracy: 0.6124\n",
      "Epoch 30/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.9626 - accuracy: 0.6804 - val_loss: 1.2444 - val_accuracy: 0.6032\n",
      "Epoch 31/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.9368 - accuracy: 0.6880 - val_loss: 1.2178 - val_accuracy: 0.6035\n",
      "Epoch 32/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.9196 - accuracy: 0.6932 - val_loss: 1.2360 - val_accuracy: 0.6064\n",
      "Epoch 33/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.9067 - accuracy: 0.6994 - val_loss: 1.2569 - val_accuracy: 0.6052\n",
      "Epoch 34/100\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.8869 - accuracy: 0.7072 - val_loss: 1.2427 - val_accuracy: 0.6033\n",
      "Epoch 35/100\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.8642 - accuracy: 0.7141 - val_loss: 1.2678 - val_accuracy: 0.6037\n",
      "Epoch 36/100\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.8421 - accuracy: 0.7226 - val_loss: 1.2088 - val_accuracy: 0.6157\n",
      "Epoch 37/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.8171 - accuracy: 0.7294 - val_loss: 1.2638 - val_accuracy: 0.6182\n",
      "Epoch 38/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.8016 - accuracy: 0.7340 - val_loss: 1.2582 - val_accuracy: 0.6110\n",
      "Epoch 39/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.7725 - accuracy: 0.7454 - val_loss: 1.3127 - val_accuracy: 0.6074\n",
      "Epoch 40/100\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.7532 - accuracy: 0.7527 - val_loss: 1.2813 - val_accuracy: 0.6124\n",
      "Epoch 41/100\n",
      "391/391 [==============================] - 29s 73ms/step - loss: 0.7342 - accuracy: 0.7581 - val_loss: 1.3031 - val_accuracy: 0.6079\n",
      "Epoch 42/100\n",
      "391/391 [==============================] - 29s 74ms/step - loss: 0.7051 - accuracy: 0.7684 - val_loss: 1.3406 - val_accuracy: 0.5965\n",
      "Epoch 43/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.6808 - accuracy: 0.7769 - val_loss: 1.3214 - val_accuracy: 0.6115\n",
      "Epoch 44/100\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.6646 - accuracy: 0.7815 - val_loss: 1.3352 - val_accuracy: 0.6153\n",
      "Epoch 45/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.6392 - accuracy: 0.7919 - val_loss: 1.4107 - val_accuracy: 0.6054\n",
      "Epoch 46/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.6187 - accuracy: 0.7985 - val_loss: 1.3857 - val_accuracy: 0.6138\n",
      "Epoch 47/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.5861 - accuracy: 0.8116 - val_loss: 1.4142 - val_accuracy: 0.5949\n",
      "Epoch 48/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.5689 - accuracy: 0.8168 - val_loss: 1.5115 - val_accuracy: 0.5978\n",
      "Epoch 49/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.5464 - accuracy: 0.8236 - val_loss: 1.3478 - val_accuracy: 0.6126\n",
      "Epoch 50/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.5232 - accuracy: 0.8339 - val_loss: 1.4925 - val_accuracy: 0.6061\n",
      "Epoch 51/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.5075 - accuracy: 0.8369 - val_loss: 1.5600 - val_accuracy: 0.6038\n",
      "Epoch 52/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.4880 - accuracy: 0.8458 - val_loss: 1.5343 - val_accuracy: 0.6043\n",
      "Epoch 53/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.4636 - accuracy: 0.8537 - val_loss: 1.6948 - val_accuracy: 0.5856\n",
      "Epoch 54/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.4568 - accuracy: 0.8545 - val_loss: 1.6602 - val_accuracy: 0.5991\n",
      "Epoch 55/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.4340 - accuracy: 0.8657 - val_loss: 1.5832 - val_accuracy: 0.6080\n",
      "Epoch 56/100\n",
      "391/391 [==============================] - 28s 72ms/step - loss: 0.4106 - accuracy: 0.8726 - val_loss: 1.6762 - val_accuracy: 0.5913\n",
      "Epoch 57/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.4020 - accuracy: 0.8750 - val_loss: 1.6250 - val_accuracy: 0.6079\n",
      "Epoch 58/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.3797 - accuracy: 0.8833 - val_loss: 1.7773 - val_accuracy: 0.5984\n",
      "Epoch 59/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.3677 - accuracy: 0.8885 - val_loss: 1.6823 - val_accuracy: 0.6106\n",
      "Epoch 60/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.3508 - accuracy: 0.8927 - val_loss: 1.6963 - val_accuracy: 0.6016\n",
      "Epoch 61/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.3351 - accuracy: 0.8985 - val_loss: 1.8089 - val_accuracy: 0.6042\n",
      "Epoch 62/100\n",
      "391/391 [==============================] - 28s 71ms/step - loss: 0.3255 - accuracy: 0.9023 - val_loss: 1.8062 - val_accuracy: 0.6082\n",
      "Epoch 63/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.3164 - accuracy: 0.9047 - val_loss: 1.8392 - val_accuracy: 0.6008\n",
      "Epoch 64/100\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.3029 - accuracy: 0.9098 - val_loss: 1.8367 - val_accuracy: 0.6041\n",
      "Epoch 65/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.2961 - accuracy: 0.9132 - val_loss: 1.8715 - val_accuracy: 0.6009\n",
      "Epoch 66/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.2888 - accuracy: 0.9148 - val_loss: 1.8994 - val_accuracy: 0.5928\n",
      "Epoch 67/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.2756 - accuracy: 0.9181 - val_loss: 1.8254 - val_accuracy: 0.6058\n",
      "Epoch 68/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.2660 - accuracy: 0.9237 - val_loss: 1.8528 - val_accuracy: 0.6057\n",
      "Epoch 69/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.2571 - accuracy: 0.9249 - val_loss: 1.9683 - val_accuracy: 0.5964\n",
      "Epoch 70/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.2461 - accuracy: 0.9289 - val_loss: 1.9837 - val_accuracy: 0.6084\n",
      "Epoch 71/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.2463 - accuracy: 0.9288 - val_loss: 2.0057 - val_accuracy: 0.5953\n",
      "Epoch 72/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.2349 - accuracy: 0.9333 - val_loss: 1.9270 - val_accuracy: 0.6147\n",
      "Epoch 73/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.2302 - accuracy: 0.9333 - val_loss: 1.9756 - val_accuracy: 0.5951\n",
      "Epoch 74/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.2217 - accuracy: 0.9362 - val_loss: 2.0772 - val_accuracy: 0.6022\n",
      "Epoch 75/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.2182 - accuracy: 0.9377 - val_loss: 2.0727 - val_accuracy: 0.6069\n",
      "Epoch 76/100\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2128 - accuracy: 0.9391 - val_loss: 2.0947 - val_accuracy: 0.6030\n",
      "Epoch 77/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.1999 - accuracy: 0.9445 - val_loss: 2.1517 - val_accuracy: 0.5891\n",
      "Epoch 78/100\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.2010 - accuracy: 0.9429 - val_loss: 2.0904 - val_accuracy: 0.5986\n",
      "Epoch 79/100\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.1957 - accuracy: 0.9456 - val_loss: 2.1782 - val_accuracy: 0.5985\n",
      "Epoch 80/100\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.1881 - accuracy: 0.9470 - val_loss: 2.2607 - val_accuracy: 0.5941\n",
      "Epoch 81/100\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.1862 - accuracy: 0.9471 - val_loss: 2.2439 - val_accuracy: 0.6005\n",
      "Epoch 82/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.1735 - accuracy: 0.9525 - val_loss: 1.9892 - val_accuracy: 0.6011\n",
      "Epoch 83/100\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.1845 - accuracy: 0.9489 - val_loss: 2.0734 - val_accuracy: 0.6067\n",
      "Epoch 84/100\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.1728 - accuracy: 0.9523 - val_loss: 1.9760 - val_accuracy: 0.6132\n",
      "Epoch 85/100\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.1704 - accuracy: 0.9533 - val_loss: 2.2266 - val_accuracy: 0.5955\n",
      "Epoch 86/100\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.1651 - accuracy: 0.9548 - val_loss: 2.1424 - val_accuracy: 0.6049\n",
      "Epoch 87/100\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.1655 - accuracy: 0.9541 - val_loss: 2.1059 - val_accuracy: 0.6092\n",
      "Epoch 88/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.1626 - accuracy: 0.9566 - val_loss: 2.2229 - val_accuracy: 0.5970\n",
      "Epoch 89/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.1599 - accuracy: 0.9562 - val_loss: 2.2212 - val_accuracy: 0.6076\n",
      "Epoch 90/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.1524 - accuracy: 0.9587 - val_loss: 2.1744 - val_accuracy: 0.6024\n",
      "Epoch 91/100\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.1481 - accuracy: 0.9600 - val_loss: 2.3895 - val_accuracy: 0.6060\n",
      "Epoch 92/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.1397 - accuracy: 0.9615 - val_loss: 2.2384 - val_accuracy: 0.6087\n",
      "Epoch 93/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.1428 - accuracy: 0.9622 - val_loss: 2.2589 - val_accuracy: 0.6040\n",
      "Epoch 94/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.1402 - accuracy: 0.9627 - val_loss: 2.2062 - val_accuracy: 0.6072\n",
      "Epoch 95/100\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.1348 - accuracy: 0.9644 - val_loss: 2.3614 - val_accuracy: 0.6008\n",
      "Epoch 96/100\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.1352 - accuracy: 0.9642 - val_loss: 2.5392 - val_accuracy: 0.6040\n",
      "Epoch 97/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.1377 - accuracy: 0.9633 - val_loss: 2.2553 - val_accuracy: 0.6120\n",
      "Epoch 98/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.1329 - accuracy: 0.9649 - val_loss: 2.2859 - val_accuracy: 0.6035\n",
      "Epoch 99/100\n",
      "391/391 [==============================] - 27s 70ms/step - loss: 0.1261 - accuracy: 0.9665 - val_loss: 2.2083 - val_accuracy: 0.5980\n",
      "Epoch 100/100\n",
      "391/391 [==============================] - 27s 69ms/step - loss: 0.1248 - accuracy: 0.9671 - val_loss: 2.3644 - val_accuracy: 0.6028\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As can be seen, the MLP doesnt perform very well for small datasets, and ends up overfitting even with a dropout probablity of 0.9.\n",
    "\n",
    "The best training/val accuracy pair occurs at epoch 84. Train acc: 0.9523, test acc: 0.6132\n",
    "\n",
    "Now, to train a similar model but to use the `Projection` layer instead of Global avg pooling, lets see if that can do any better in this scenario"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "history1 = history"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def create_model():\n",
    "    dims = 512\n",
    "    k, s = (4, 4)  # (kernel, strides)\n",
    "    depth = 4\n",
    "\n",
    "    inputs = Input((32, 32, 3))\n",
    "\n",
    "    x = tf.keras.Sequential(\n",
    "        [\n",
    "            RandomFlip(),\n",
    "            RandomRotation(factor=0.03),\n",
    "            RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "        ],\n",
    "        name=\"data_augmentation\",\n",
    "    )(inputs)\n",
    "\n",
    "    x = CreatePatches(k, s)(x)\n",
    "    x = PerPatchFullyConnected(dims)(x)\n",
    "\n",
    "    for _ in range(depth):\n",
    "        x = MLPBlock()(x)\n",
    "        x = Permute((2,1))(x)\n",
    "\n",
    "    # x = Projection()(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "\n",
    "    # classification layer\n",
    "    x = GaussianDropout(0.9)(x)\n",
    "    output = Dense(len(classes), activation=\"softmax\", kernel_regularizer=\"l2\")(x)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=output)\n",
    "\n",
    "\n",
    "model = create_model()\n",
    "print(model.summary())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "data_augmentation (Sequentia (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "create_patches (CreatePatche (None, 8, 8, 48)          0         \n",
      "_________________________________________________________________\n",
      "per_patch_fully_connected (P (None, 64, 512)           25088     \n",
      "_________________________________________________________________\n",
      "mlp_block (MLPBlock)         (None, 64, 512)           535680    \n",
      "_________________________________________________________________\n",
      "permute (Permute)            (None, 512, 64)           0         \n",
      "_________________________________________________________________\n",
      "mlp_block_1 (MLPBlock)       (None, 512, 64)           533888    \n",
      "_________________________________________________________________\n",
      "permute_1 (Permute)          (None, 64, 512)           0         \n",
      "_________________________________________________________________\n",
      "mlp_block_2 (MLPBlock)       (None, 64, 512)           535680    \n",
      "_________________________________________________________________\n",
      "permute_2 (Permute)          (None, 512, 64)           0         \n",
      "_________________________________________________________________\n",
      "mlp_block_3 (MLPBlock)       (None, 512, 64)           533888    \n",
      "_________________________________________________________________\n",
      "permute_3 (Permute)          (None, 64, 512)           0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "gaussian_dropout (GaussianDr (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 2,169,354\n",
      "Trainable params: 2,169,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    optimizer=tf.keras.optimizers.Adadelta(1),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# b_s = 256 # batch_size\n",
    "\n",
    "# val_dataset = val_dataset.batch(b_s)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    validation_data=val_dataset,\n",
    "    batch_size=256,\n",
    "    epochs=100,\n",
    "    verbose=\"auto\",\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-07-21 20:50:25.545241: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-07-21 20:50:25.565189: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 3800060000 Hz\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2021-07-21 20:50:27.291970: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2021-07-21 20:50:27.643179: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2021-07-21 20:50:27.643840: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2021-07-21 20:50:28.000018: I tensorflow/stream_executor/cuda/cuda_dnn.cc:359] Loaded cuDNN version 8202\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "196/196 [==============================] - 42s 203ms/step - loss: 2.5711 - accuracy: 0.1962 - val_loss: 2.0845 - val_accuracy: 0.2584\n",
      "Epoch 2/100\n",
      "196/196 [==============================] - 40s 203ms/step - loss: 2.0048 - accuracy: 0.2985 - val_loss: 1.7510 - val_accuracy: 0.3821\n",
      "Epoch 3/100\n",
      "196/196 [==============================] - 39s 200ms/step - loss: 1.7978 - accuracy: 0.3626 - val_loss: 1.6731 - val_accuracy: 0.3975\n",
      "Epoch 4/100\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 1.6949 - accuracy: 0.3971 - val_loss: 1.6392 - val_accuracy: 0.4074\n",
      "Epoch 5/100\n",
      "196/196 [==============================] - 33s 169ms/step - loss: 1.6316 - accuracy: 0.4203 - val_loss: 1.5224 - val_accuracy: 0.4675\n",
      "Epoch 6/100\n",
      "196/196 [==============================] - 32s 166ms/step - loss: 1.5780 - accuracy: 0.4400 - val_loss: 1.5047 - val_accuracy: 0.4550\n",
      "Epoch 7/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 1.5371 - accuracy: 0.4545 - val_loss: 1.5301 - val_accuracy: 0.4681\n",
      "Epoch 8/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 1.5056 - accuracy: 0.4680 - val_loss: 1.4270 - val_accuracy: 0.5016\n",
      "Epoch 9/100\n",
      "196/196 [==============================] - 32s 166ms/step - loss: 1.4608 - accuracy: 0.4869 - val_loss: 1.4123 - val_accuracy: 0.5042\n",
      "Epoch 10/100\n",
      "196/196 [==============================] - 32s 166ms/step - loss: 1.4290 - accuracy: 0.4982 - val_loss: 1.3773 - val_accuracy: 0.5160\n",
      "Epoch 11/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 1.3972 - accuracy: 0.5092 - val_loss: 1.3311 - val_accuracy: 0.5313\n",
      "Epoch 12/100\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 1.3751 - accuracy: 0.5197 - val_loss: 1.3095 - val_accuracy: 0.5368\n",
      "Epoch 13/100\n",
      "196/196 [==============================] - 33s 168ms/step - loss: 1.3491 - accuracy: 0.5292 - val_loss: 1.3321 - val_accuracy: 0.5262\n",
      "Epoch 14/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 1.3247 - accuracy: 0.5390 - val_loss: 1.3323 - val_accuracy: 0.5268\n",
      "Epoch 15/100\n",
      "196/196 [==============================] - 32s 166ms/step - loss: 1.3136 - accuracy: 0.5446 - val_loss: 1.3354 - val_accuracy: 0.5310\n",
      "Epoch 16/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 1.2862 - accuracy: 0.5526 - val_loss: 1.4018 - val_accuracy: 0.5153\n",
      "Epoch 17/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 1.2681 - accuracy: 0.5638 - val_loss: 1.2935 - val_accuracy: 0.5490\n",
      "Epoch 18/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 1.2526 - accuracy: 0.5684 - val_loss: 1.4621 - val_accuracy: 0.5042\n",
      "Epoch 19/100\n",
      "196/196 [==============================] - 32s 166ms/step - loss: 1.2336 - accuracy: 0.5763 - val_loss: 1.3140 - val_accuracy: 0.5498\n",
      "Epoch 20/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 1.2167 - accuracy: 0.5827 - val_loss: 1.2587 - val_accuracy: 0.5627\n",
      "Epoch 21/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 1.2039 - accuracy: 0.5898 - val_loss: 1.2740 - val_accuracy: 0.5603\n",
      "Epoch 22/100\n",
      "196/196 [==============================] - 32s 166ms/step - loss: 1.1811 - accuracy: 0.5941 - val_loss: 1.1674 - val_accuracy: 0.5924\n",
      "Epoch 23/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 1.1659 - accuracy: 0.6037 - val_loss: 1.1691 - val_accuracy: 0.5921\n",
      "Epoch 24/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 1.1450 - accuracy: 0.6112 - val_loss: 1.2027 - val_accuracy: 0.5842\n",
      "Epoch 25/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 1.1324 - accuracy: 0.6132 - val_loss: 1.2905 - val_accuracy: 0.5559\n",
      "Epoch 26/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 1.1093 - accuracy: 0.6205 - val_loss: 1.1568 - val_accuracy: 0.6025\n",
      "Epoch 27/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 1.0907 - accuracy: 0.6291 - val_loss: 1.2802 - val_accuracy: 0.5632\n",
      "Epoch 28/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 1.0819 - accuracy: 0.6332 - val_loss: 1.3765 - val_accuracy: 0.5598\n",
      "Epoch 29/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 1.0540 - accuracy: 0.6390 - val_loss: 1.3208 - val_accuracy: 0.5738\n",
      "Epoch 30/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 1.0399 - accuracy: 0.6481 - val_loss: 1.4719 - val_accuracy: 0.5177\n",
      "Epoch 31/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 1.0173 - accuracy: 0.6559 - val_loss: 1.3326 - val_accuracy: 0.5644\n",
      "Epoch 32/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.9932 - accuracy: 0.6670 - val_loss: 1.1824 - val_accuracy: 0.6014\n",
      "Epoch 33/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.9804 - accuracy: 0.6680 - val_loss: 1.2722 - val_accuracy: 0.5875\n",
      "Epoch 34/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.9554 - accuracy: 0.6769 - val_loss: 1.3986 - val_accuracy: 0.5612\n",
      "Epoch 35/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.9374 - accuracy: 0.6843 - val_loss: 1.2595 - val_accuracy: 0.5936\n",
      "Epoch 36/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.9250 - accuracy: 0.6900 - val_loss: 1.3367 - val_accuracy: 0.5797\n",
      "Epoch 37/100\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.9010 - accuracy: 0.6947 - val_loss: 1.3385 - val_accuracy: 0.5751\n",
      "Epoch 38/100\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.8811 - accuracy: 0.7028 - val_loss: 1.2353 - val_accuracy: 0.6035\n",
      "Epoch 39/100\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.8562 - accuracy: 0.7126 - val_loss: 1.3357 - val_accuracy: 0.5853\n",
      "Epoch 40/100\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.8287 - accuracy: 0.7219 - val_loss: 1.2488 - val_accuracy: 0.6163\n",
      "Epoch 41/100\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.8185 - accuracy: 0.7258 - val_loss: 1.2763 - val_accuracy: 0.5898\n",
      "Epoch 42/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.7852 - accuracy: 0.7396 - val_loss: 1.3418 - val_accuracy: 0.5974\n",
      "Epoch 43/100\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.7666 - accuracy: 0.7436 - val_loss: 1.3012 - val_accuracy: 0.5889\n",
      "Epoch 44/100\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.7448 - accuracy: 0.7516 - val_loss: 1.2503 - val_accuracy: 0.6000\n",
      "Epoch 45/100\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.7236 - accuracy: 0.7592 - val_loss: 1.4388 - val_accuracy: 0.5961\n",
      "Epoch 46/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.7057 - accuracy: 0.7652 - val_loss: 1.4240 - val_accuracy: 0.5798\n",
      "Epoch 47/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.6791 - accuracy: 0.7750 - val_loss: 1.4783 - val_accuracy: 0.5890\n",
      "Epoch 48/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.6580 - accuracy: 0.7827 - val_loss: 1.3119 - val_accuracy: 0.6045\n",
      "Epoch 49/100\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.6318 - accuracy: 0.7913 - val_loss: 1.5618 - val_accuracy: 0.5730\n",
      "Epoch 50/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.6201 - accuracy: 0.7948 - val_loss: 1.3965 - val_accuracy: 0.5959\n",
      "Epoch 51/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.5923 - accuracy: 0.8077 - val_loss: 1.5761 - val_accuracy: 0.5830\n",
      "Epoch 52/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.5742 - accuracy: 0.8120 - val_loss: 1.4377 - val_accuracy: 0.6011\n",
      "Epoch 53/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.5542 - accuracy: 0.8216 - val_loss: 1.5904 - val_accuracy: 0.5780\n",
      "Epoch 54/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.5410 - accuracy: 0.8255 - val_loss: 1.4854 - val_accuracy: 0.5942\n",
      "Epoch 55/100\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.5140 - accuracy: 0.8335 - val_loss: 1.4433 - val_accuracy: 0.6041\n",
      "Epoch 56/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.4994 - accuracy: 0.8395 - val_loss: 1.4417 - val_accuracy: 0.5996\n",
      "Epoch 57/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.4797 - accuracy: 0.8465 - val_loss: 1.5897 - val_accuracy: 0.5713\n",
      "Epoch 58/100\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.4617 - accuracy: 0.8500 - val_loss: 1.6618 - val_accuracy: 0.5871\n",
      "Epoch 59/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.4452 - accuracy: 0.8568 - val_loss: 1.6417 - val_accuracy: 0.5973\n",
      "Epoch 60/100\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.4268 - accuracy: 0.8635 - val_loss: 1.7307 - val_accuracy: 0.5925\n",
      "Epoch 61/100\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.4148 - accuracy: 0.8702 - val_loss: 1.8774 - val_accuracy: 0.5678\n",
      "Epoch 62/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.4028 - accuracy: 0.8726 - val_loss: 1.7966 - val_accuracy: 0.5814\n",
      "Epoch 63/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.3930 - accuracy: 0.8751 - val_loss: 1.6444 - val_accuracy: 0.6070\n",
      "Epoch 64/100\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.3699 - accuracy: 0.8840 - val_loss: 1.8508 - val_accuracy: 0.5951\n",
      "Epoch 65/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.3669 - accuracy: 0.8853 - val_loss: 1.6527 - val_accuracy: 0.5954\n",
      "Epoch 66/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.3533 - accuracy: 0.8892 - val_loss: 1.7803 - val_accuracy: 0.5932\n",
      "Epoch 67/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.3417 - accuracy: 0.8933 - val_loss: 2.0248 - val_accuracy: 0.5799\n",
      "Epoch 68/100\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.3309 - accuracy: 0.8972 - val_loss: 1.7308 - val_accuracy: 0.5954\n",
      "Epoch 69/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.3146 - accuracy: 0.9023 - val_loss: 1.8726 - val_accuracy: 0.5872\n",
      "Epoch 70/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.3115 - accuracy: 0.9036 - val_loss: 1.9216 - val_accuracy: 0.5775\n",
      "Epoch 71/100\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.3027 - accuracy: 0.9072 - val_loss: 2.0825 - val_accuracy: 0.5902\n",
      "Epoch 72/100\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.2922 - accuracy: 0.9104 - val_loss: 2.0320 - val_accuracy: 0.5805\n",
      "Epoch 73/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.2901 - accuracy: 0.9113 - val_loss: 2.0680 - val_accuracy: 0.5855\n",
      "Epoch 74/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.2775 - accuracy: 0.9150 - val_loss: 1.8373 - val_accuracy: 0.5865\n",
      "Epoch 75/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.2705 - accuracy: 0.9185 - val_loss: 1.9856 - val_accuracy: 0.5899\n",
      "Epoch 76/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.2655 - accuracy: 0.9195 - val_loss: 1.9688 - val_accuracy: 0.5925\n",
      "Epoch 77/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.2592 - accuracy: 0.9217 - val_loss: 1.9217 - val_accuracy: 0.5858\n",
      "Epoch 78/100\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.2461 - accuracy: 0.9256 - val_loss: 1.9544 - val_accuracy: 0.5889\n",
      "Epoch 79/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.2443 - accuracy: 0.9277 - val_loss: 1.9533 - val_accuracy: 0.6068\n",
      "Epoch 80/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.2367 - accuracy: 0.9291 - val_loss: 1.9749 - val_accuracy: 0.5958\n",
      "Epoch 81/100\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.2332 - accuracy: 0.9304 - val_loss: 2.0620 - val_accuracy: 0.5935\n",
      "Epoch 82/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.2223 - accuracy: 0.9353 - val_loss: 2.0985 - val_accuracy: 0.5839\n",
      "Epoch 83/100\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.2236 - accuracy: 0.9353 - val_loss: 2.0370 - val_accuracy: 0.5848\n",
      "Epoch 84/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.2148 - accuracy: 0.9375 - val_loss: 2.0315 - val_accuracy: 0.5996\n",
      "Epoch 85/100\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.2102 - accuracy: 0.9394 - val_loss: 2.0601 - val_accuracy: 0.5924\n",
      "Epoch 86/100\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.2115 - accuracy: 0.9372 - val_loss: 1.9197 - val_accuracy: 0.5932\n",
      "Epoch 87/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.2000 - accuracy: 0.9397 - val_loss: 2.2958 - val_accuracy: 0.5937\n",
      "Epoch 88/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.2034 - accuracy: 0.9413 - val_loss: 2.1031 - val_accuracy: 0.5975\n",
      "Epoch 89/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.1907 - accuracy: 0.9454 - val_loss: 2.2101 - val_accuracy: 0.5845\n",
      "Epoch 90/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.1914 - accuracy: 0.9442 - val_loss: 1.9653 - val_accuracy: 0.6019\n",
      "Epoch 91/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.1901 - accuracy: 0.9449 - val_loss: 2.2063 - val_accuracy: 0.5939\n",
      "Epoch 92/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.1789 - accuracy: 0.9491 - val_loss: 2.1957 - val_accuracy: 0.5964\n",
      "Epoch 93/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.1768 - accuracy: 0.9489 - val_loss: 2.0685 - val_accuracy: 0.5948\n",
      "Epoch 94/100\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.1780 - accuracy: 0.9491 - val_loss: 2.3398 - val_accuracy: 0.5814\n",
      "Epoch 95/100\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.1731 - accuracy: 0.9500 - val_loss: 2.2421 - val_accuracy: 0.5943\n",
      "Epoch 96/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.1637 - accuracy: 0.9538 - val_loss: 2.2041 - val_accuracy: 0.5814\n",
      "Epoch 97/100\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.1677 - accuracy: 0.9526 - val_loss: 2.1550 - val_accuracy: 0.5952\n",
      "Epoch 98/100\n",
      "196/196 [==============================] - 33s 167ms/step - loss: 0.1621 - accuracy: 0.9536 - val_loss: 2.2053 - val_accuracy: 0.5888\n",
      "Epoch 99/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.1604 - accuracy: 0.9551 - val_loss: 2.3207 - val_accuracy: 0.5629\n",
      "Epoch 100/100\n",
      "196/196 [==============================] - 33s 166ms/step - loss: 0.1520 - accuracy: 0.9578 - val_loss: 2.3589 - val_accuracy: 0.5964\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('MLP-Mixer': venv)"
  },
  "interpreter": {
   "hash": "3339bacc6ae8024b829b40334f20da644df689ab7b13cd733df3f409221c3802"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}